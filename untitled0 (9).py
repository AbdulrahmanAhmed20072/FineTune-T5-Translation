# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ltR4hJZap1VyEtDRnm3PUaYRuFo1OxCE
"""

!pip install -q datasets==3.2.0 bitsandbytes==0.45.1 accelerate==1.2.1 evaluate==0.4.3 \
    transformers==4.47.1 torch==2.5.1+cu124 numpy==1.26.4 peft==0.14.0 sacrebleu

import os
import torch
import numpy as np
import evaluate
from datasets import load_dataset
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    BitsAndBytesConfig
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    prepare_model_for_kbit_training
)

# Set random seed for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# Configurations
MODEL_NAME = "google-t5/t5-small"
TRANSLATION_PREFIX = "translate from English to French: "
MAX_LENGTH = 128
BATCH_SIZE = 16
NUM_TRAIN_EPOCHS = 3
LEARNING_RATE = 2e-4

# Load dataset
books = load_dataset("opus_books", "en-fr", split = "train[:1000]")
books = books.train_test_split(test_size = 0.2, seed = 42)
books

# translation sample example
books["train"]["translation"][0]

# Initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def preprocessing_function(examples):

    """
    Preprocess the translation dataset by tokenizing inputs and targets.

    Args:
        examples (dict): Batch of translation examples

    Returns:
        dict: Tokenized model inputs
    """

    inputs = [TRANSLATION_PREFIX + ex["en"] for ex in examples["translation"]]
    targets = [ex["fr"] for ex in examples["translation"]]

    model_inputs = tokenizer(
        inputs,
        text_target = targets,
        max_length = MAX_LENGTH,
        truncation = True
    )

    return model_inputs

# Tokenize dataset
tokenized_books = books.map(
    preprocessing_function,
    batched = True,
    remove_columns = books["train"].column_names
)

# Data collator
data_collator = DataCollatorForSeq2Seq(
    tokenizer = tokenizer,
    model = MODEL_NAME,
    return_tensors = "pt"
)

# Load evaluation metric
metric = evaluate.load("sacrebleu")

def postprocess_text(preds, labels):

    """
    Postprocess predictions and labels for metric computation.

    Args:
        preds (list): Model predictions
        labels (list): Ground truth labels

    Returns:
        tuple: Processed predictions and labels
    """

    preds = [pred.strip() for pred in preds]
    labels = [[label.strip()] for label in labels]

    return preds, labels

def compute_metrics(eval_pred):
    """
    Compute translation metrics.

    Args:
        eval_pred (EvalPrediction): Model predictions and labels

    Returns:
        dict: Evaluation metrics
    """
    preds, labels = eval_pred

    # Decode predictions and labels
    if isinstance(preds, tuple):
        preds = preds[0]

    # Convert to token IDs
    decoded_preds = tokenizer.batch_decode(
        preds.argmax(-1),
        skip_special_tokens=True
    )

    # Handle labels (replace -100 with pad token)
    decoded_labels = tokenizer.batch_decode(
        np.where(labels != -100, labels, tokenizer.pad_token_id), # where labels equal -100 replace with pad token
        skip_special_tokens=True
    )

    # Postprocess
    decoded_preds, decoded_labels = postprocess_text(
        decoded_preds, decoded_labels
    )

    # Compute BLEU score
    result = metric.compute(
        predictions = decoded_preds,
        references = decoded_labels
    )

    # Add generation length metric
    prediction_lens = [
        np.count_nonzero(pred) for pred in preds.argmax(-1)
    ]

    return {
        "bleu": result["score"],
        "gen_len": np.mean(prediction_lens),
        "bleu_normalized": round(result["score"], 4)
    }

# Quantization configuration
quantization_config = BitsAndBytesConfig(
    load_in_4bit = True ,
    bnb_4bit_compute_dtype = 'bfloat16',
    bnb_4bit_quant_type = 'nf4',
    bnb_4bit_use_double_quant = True ,
)

# Load base model with quantization
base_model = AutoModelForSeq2SeqLM.from_pretrained(
    MODEL_NAME,
    quantization_config = quantization_config,
)

# insted of storing activations in ram to be used in backword pass
# save some and calculate the other during backword pass, this saves some memory
base_model.gradient_checkpointing_enable()

# Prepare model for k-bit training
model = prepare_model_for_kbit_training(base_model)

# Find target modules for LoRA
target_modules = set()
for name, module in base_model.named_modules():
    if isinstance(module, torch.nn.Linear):
        module_name = name.split('.')[-1]
        target_modules.add(module_name)
target_modules = list(target_modules)
target_modules

# Configure LoRA
lora_config = LoraConfig(
    r = 4,
    lora_alpha = 2,
    lora_dropout = 0.1,
    bias = 'none',
    task_type = TaskType.SEQ_2_SEQ_LM,
    target_modules = target_modules
)

# Get PEFT model
lora_model = get_peft_model(model, lora_config)
lora_model.print_trainable_parameters()

# Training arguments
training_args = TrainingArguments(
    output_dir = "./translation_results",
    overwrite_output_dir = True,
    num_train_epochs = NUM_TRAIN_EPOCHS,
    per_device_train_batch_size = BATCH_SIZE,
    per_device_eval_batch_size = BATCH_SIZE,
    learning_rate = LEARNING_RATE,
    warmup_ratio = 0.1,
    weight_decay = 0.01, # L2 regularization
    logging_dir = './logs',
    logging_strategy = 'steps',
    logging_steps = 50,
    evaluation_strategy = 'epoch',
    save_strategy = 'epoch',
    load_best_model_at_end = True,
    metric_for_best_model = 'bleu',
    dataloader_pin_memory = True, # data intially loadded into CPU then copied to GPU, set to True for transfer directly to GPU, it's faster
    bf16 = True,
    push_to_hub = False,  # Set to True if you want to push to HuggingFace Hub
)

# Initialize Trainer
trainer = Trainer(
    model = lora_model,
    args = training_args,
    train_dataset = tokenized_books['train'],
    eval_dataset = tokenized_books['test'],
    data_collator = data_collator,
    compute_metrics = compute_metrics,
    processing_class = tokenizer
)

# Train the model
trainer.train()

# Inference function
def translate_text(text, max_length=MAX_LENGTH):

    """
    Translate text from English to French.

    Args:
        text (str): Input text to translate
        max_length (int): Maximum length of generated translation

    Returns:
        str: Translated text
    """

    # Prepare input
    inputs = tokenizer(
        TRANSLATION_PREFIX + text,
        return_tensors = "pt",
        max_length = max_length,
        truncation = True
    ).to(lora_model.device)

    # Set model to evaluation mode
    lora_model.eval()

    # Generate translation
    with torch.no_grad():
        outputs = lora_model.generate(
            **inputs,
            max_length = max_length,
            num_return_sequences = 1,
            do_sample = False  # Use greedy decoding
        )

    # Decode and return translation
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Example usage
example_text = "Hello, how are you?"

print(f"Original: {example_text}")
print(f"Translation: {translate_text(example_text)}")